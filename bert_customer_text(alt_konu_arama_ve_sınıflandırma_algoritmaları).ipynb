{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6jgnr9wPP8V"
      },
      "source": [
        "# kütüphane yükle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFy6Zp4EqSxG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKRC5BTkpckd"
      },
      "source": [
        "!pip install bertopic\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "!pip install tokenizers\n",
        "\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install wordcloud\n",
        "\n",
        ">pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8lC-RaYvbyO"
      },
      "outputs": [],
      "source": [
        "#bertopic kullanmak için\n",
        "!pip install bertopic\n",
        "!pip install sentencepiece\n",
        "from bertopic import BERTopic\n",
        "from transformers import AutoModel, AutoTokenizer # bert için içeri aktardığımız önceden eğitilmiş embedding modeli kendi modelimizde kullanabilmek için bazı işlemlerden geçirtmek için bu kütüphaneden yararlanılır.\n",
        "import pandas as pd # veriyi excelden almak ve veriyle önişlemler gerçekleşitrmek için kütüphane\n",
        "from wordcloud import WordCloud # veride geçen en çok kelimeleri görselleştirmek için kelime bulutu kütüphanesi\n",
        "import matplotlib.pyplot as plt # grafik oluşturmak için kütüphane\n",
        " # konu modellemek için gensim'in bazı modüllerinden yararlanılır.\n",
        "import gensim.corpora as corpora # veriyi lda modelinde kullanmak için sözlük modülü\n",
        "from gensim.models.coherencemodel import CoherenceModel # modeli değerlendirmek için modül\n",
        "import nltk # türkçe stopwords'leri kullanmak için\n",
        "import re # yazıları daha anlaşılır yapmak için veri değiştirme kütüphanesi\n",
        "from nltk.tokenize import RegexpTokenizer # Cümleleri makine için daha anlaşılır yapmak için bölmek için modül\n",
        "from collections import Counter # veride geçen kelimeleri saydırmak için kütüphane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8VrS9XM4SSE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TOPm5TTPTZY"
      },
      "source": [
        "# veri yükle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4IDwHBaR7Ej"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Klasör yolu - Check this path for typos and ensure the directory exists\n",
        "folder_path = '/content/drive/My Drive'\n",
        "\n",
        "# List the contents of 'My Drive' to verify the 'Metin' directory exists\n",
        "!ls \"/content/drive/My Drive\"\n",
        "\n",
        "# Klasörü aç - This line should work if the path is correct and the drive is mounted\n",
        "os.chdir(folder_path)\n",
        "\n",
        "# Klasör içeriğini listele\n",
        "print(os.listdir(folder_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFarMby_VCLN"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_excel('CallToTexNewData.xlsx')\n",
        "df2 = pd.read_excel('CallToTexNewData1.xlsx')\n",
        "df3 = pd.read_excel('CallToTexNewData2.xlsx')\n",
        "df4 = pd.read_excel('CallToTexNewData3.xlsx')\n",
        "df5 = pd.read_excel('Kayıt Detay2.xlsx')\n",
        "df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
        "\n",
        "# sadece müşteri çağrıları ve ürünlerle çalışağımızdan aşağıdaki veriler verisetinden çıkarılır.\n",
        "df.drop(columns=['CallId','Kuyruk','AgentText', 'Id','AgentTextTimes','CustomerTextTimes','Type'], inplace=True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Yeni Ürün Tanıtımı'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Hizmet Takibi Servis Takibi'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Kurumsal Satış'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Kurumsal Satış - Telefon'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'İngilizce Çağrılar'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Memnuniyet Araması'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Uzaktan Erişim'].index, inplace = True)\n",
        "df.drop(df[df['Kuyruk Konusu'] == 'Çözüm Ekibi'].index, inplace = True)\n",
        "df.drop(columns=['Kuyruk Konusu'], inplace=True)\n",
        "\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIOaIBnjPXq6"
      },
      "source": [
        "# veri önişle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H304aseN0fV-"
      },
      "outputs": [],
      "source": [
        "# Müşterilerin söylediklerinde geçen en çok 100 kelime\n",
        "Counter(\" \".join(df[\"CustomerText\"]).split()).most_common(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHrBhiJs_BYl"
      },
      "outputs": [],
      "source": [
        "# kelime bulutu en çok 250 kelime\n",
        "long_string = ','.join(list(df['CustomerText'].values))\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=250, width=700, height=500, contour_width=10, contour_color='steelblue')\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1HVYrzdvixB"
      },
      "outputs": [],
      "source": [
        "# Kelimelerdeki yazım yanlışlarını düzeltme\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'birim[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'birim', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'tablet[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'tablet', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'telefon[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'telefon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'televizyon[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'televizyon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'makine[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'makine', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'şifre[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'şifre', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ekran[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'ekran', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'garanti[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'garanti', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ar\\u0131za[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'arıza', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r's\\u00fcp\\u00fcrge[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'süpürge', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'siz[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'siz', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'do\\u011fru[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'doğru', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'gel[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'gel', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'gir[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'gir', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'tamam[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'tamam', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'biz[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'biz', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ben[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'ben', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ses[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'ses', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ayar[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'ayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'uygulama[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'uygulama', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'numara[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'numara', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'bey[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bey', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'bilgisayar[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'buzdola[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'buzdolap', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ankast[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'ankastre', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'davlumbaz[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'davlumbaz', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'f\\u0131r\\u0131n[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'fırın', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'mikrodalga[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'mikro dalga', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'mikrodalga', 'mikro dalga', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'mikro dalga[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'mikro dalga', regex=True)\n",
        "\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'set \\u00fcst\\u00fc[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'set üstü', regex=True)\n",
        "\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'bozu[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'arıza', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'buz dola[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'buzdolap', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'buluray[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bluray', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'eniks[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'nx', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'enx[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'nx', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'enix[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'nx', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'klima[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'klima', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'diz\\u00fcst\\u00fc[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'diz \\u00fcst\\u00fc[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'laptop[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'notebook[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'faks[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'faks', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'yaz\\u0131c\\u0131[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'yazıcı', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'taray\\u0131c\\u0131[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'tarayıcı', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\u00e7amas\\u0131r[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'çamaşır', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'kurutma[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'kurutma', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'makina[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'makine', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'telekom[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'telekom', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\u015fikayet[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'şikayet', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'cihaz[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'cihaz', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\bara[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'arama', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'internet[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'internet', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'mesaj[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'mesaj', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btel\\b', 'telefon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btelef\\b', 'telefon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'mobil[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'mobil', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btel\\b', 'telefon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'bula\\u015f\\u0131k[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'bulaşık', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'sistem[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'sistem', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\bpc\\b', 'bilgisayar', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'ses sistem', 'ev sinema sistem', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'sinema sistem', 'ev sinema sistem', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\bkayd', 'kayıt', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btv\\'[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'tv', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btv\\b', 'televizyon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btelevizy\\b', 'televizyon', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\btrol\\b', 'kontrol', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'batarya[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'batarya', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'm\\*taj', 'montaj', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'montaj[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'montaj', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'm\\*it\\u00f6r', 'monitör', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'monit\\u00f6r[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'monitör', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\u015farj[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'şarj', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'servis[a-z+\\u0131+\\u011f+\\u011e+\\u0131+\\u0130+\\u00f6+\\u00d6+\\u00fc+\\u00dc+\\u015f+\\u015e+\\u00e7+\\u00c7]+', 'servis', regex=True)\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\bkod', 'kod', regex=True)\n",
        "\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4F4etOvwyZj"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+') # Cümleleri küçük ifadelere böler değişkeni tanımlanır.\n",
        "punct_re=lambda x :\" \".join(tokenizer.tokenize(x.lower())) # Verileri küçük harflere dönüştür değişkeni tanımlanır.\n",
        "\n",
        "nltk.download('stopwords') # nltk'den stopwords indirilir. Veri ne kadar sade olsa o kadar iyidir bundan dolayı cümleye anlam katmayan gereksiz kelimelerden(stopword) kurtulmak istenir.\n",
        "stop_word_list = nltk.corpus.stopwords.words('turkish')  # Türkçe stopword'leri bir değişkene atanır\n",
        "\n",
        "# stopword'leri çıkarmak için fonksiyon\n",
        "def stopword_extraction(values):\n",
        "    wordFilter = [word for word in values.split() if word not in stop_word_list]\n",
        "    notStopword = \" \".join(wordFilter)\n",
        "    return notStopword\n",
        "\n",
        "df[\"CustomerText\"] = df[\"CustomerText\"].apply(punct_re) # metin verileri küçük harflere çevir\n",
        "df['CustomerText'] = df['CustomerText'].apply(lambda x: stopword_extraction(x)) # metin verilerinden stopword'leri çıkar\n",
        "\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'\\S*@\\S*\\s?', '', regex=True) # e-posta sil\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'[^\\w\\s]', '', regex=True) # noktalama işaretlerini sil\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'http\\S+', '', regex=True) # bağlantı sil\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(r'www\\S+', '', regex=True) # bağlantı sil\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-ZLPQb6fpvd"
      },
      "outputs": [],
      "source": [
        "# modelin anlamsız sonuçlar doğurmasını engellemek için herhangi bir kelimeye bağlı olmayan kısa harf/hece/kelime çıkarılır.\n",
        "remove_words =['ha','haa', 'ben', 'mi','yok', 'var', 'an', 'hı', 'alo','dan','ç','f','d', 'gün', 'onu','ben','mı' 'mi','anda','size','oldu','olur','r',\n",
        "               'he', 'la', 'den','olab','bak','hah','ba','hım','gir','gel','siz','biz','ilg','tl','baska','evet', 'ııı','eee','tamam','simdi', 'falan', 'zaten', 'mesela', 'aaa', 'herhalde','tabi', 'yine',\n",
        "                'kolay', 'gelsin', 'teşekkür', 'ederim', 'dakika', 'bir', 'saniye', 'günler', 'iyi', 'akşamlar',\n",
        "                'vallahi', 'hani', 'işte', 'efendim', 'canım', 'peki', 'abi', 'hıhı', 'sağolun','hehe', 'buyurun','hanım',\n",
        "                'benim','zaman', 'bey', 'söyle', 'diyor', 'değil', 'bana', 'sizi', 'sonra', 'bunu','anladım','hayır',\n",
        "                'teşekkürler', 'böyle', 'öyle','bilmiyorum', 'burada','bunun','dediğim', 'aynen','tekrar','günaydın',\n",
        "                'önce', 'şeyi','dedi','bende', 'sadece','diğer','orada','sağolasın','rica','ulaştı','valla', 'bugün',\n",
        "                'bağlayab','misiniz', 'hanımefendi', 'abicim', 'kızım', 'hocam', 'abla', 'ablacığım', 'inşallah', 'hayırlı','hayırlı işler',\n",
        "                'beyefendi', 'şekilde', 'kardeşim', 'dediler', 'ismim', 'hemen', 'diyorum','bakın','bes yüz','e','merhaba','pard','un','bin yüz',\n",
        "                'pardon','doğru', 'merhabalar','gerçekten', 'şey', 'çünkü','s','ra','nasıl', 'u','hoşçakalın','müsait','çok', 'um','umu',\n",
        "               'trol edelim','miyim','murat','ahmet','şöyle','olabilir','eğer','fatih','hakan','ayse','elif','gökhan','deniz',\n",
        "               'can', 'bes','öz_gi','tan','yüz','bin','yetmiş','seksen','doksan','atmış','tane','uda','nasılsınız','için','heh','aa','si','sanırım','artık',\n",
        "               'sağol','ay','olarak', 'almıştım','aldık','aldım','hee','no','on','sen','hiç','şeyler','bunlar','söyleyeyim','dedim',\n",
        "               'şimdi','arama','musunuz','başka','beş','model','kod','ait','dün','yarın','numara','değ','allah','umun','tire','cihaz']\n",
        "\n",
        "sil = r'\\b(?:{})\\b'.format('|'.join(remove_words))\n",
        "df['CustomerText'] = df['CustomerText'].str.replace(sil, '', regex=True)\n",
        "df.drop(df[df['CustomerText'].map(len) < 10].index, inplace=True) # metin verilerinde 10'dan az karaktere sahipleri sil\n",
        "\n",
        "# index düzelt\n",
        "df.reset_index(inplace=True)\n",
        "df.set_index('index')\n",
        "df.drop(columns=['index'], inplace=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU1gYgVjPod7"
      },
      "outputs": [],
      "source": [
        "# kelime bulutu en çok 250 kelime\n",
        "long_string = ','.join(list(df['CustomerText'].values))\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=250, width=700, height=500, contour_width=10, contour_color='steelblue')\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJQnz8_kPdjf"
      },
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUVeRDGy-nHf"
      },
      "outputs": [],
      "source": [
        "# BERT, embedding model'e ihtiyaç duyar. embedding model olarak bert base turkish kullanıldı\n",
        "# Önceden eğitilmiş modeli BERT'te kullanmak için otomatik sınıflar kullanılarak ilk önce tokenize sonra model oluşturulur.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")\n",
        "model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWxmm7v_emNM"
      },
      "source": [
        "# ilk yaptıklarıModel tanımlanır ve çalıştırılır.\n",
        "topic_model = BERTopic(embedding_model=model, diversity=0.7,\n",
        "                       language='turkish', nr_topics=14, n_gram_range=(1,2), verbose=True,\n",
        "                       calculate_probabilities=True, seed_topic_list=[['ankastre','fırın','set üstü','davlumbaz','mikro dalga'],['bulaşık'],['buzdolap'],['telefon'],['süpürge'],['sinema sistem'],\n",
        "                                                                                    ['fotoğraf makine'],['klima'],['monitör'],['bilgisayar'],['televizyon'],['yazıcı'],['çamaşır'],['tablet']])\n",
        "topics, probs = topic_model.fit_transform(df.CustomerText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV3Tl8QZe5ie"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mG5iaqHPedqW"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "seed_topic_list = [\n",
        "    ['ankastre', 'fırın', 'set', 'üstü', 'davlumbaz', 'mikro', 'dalga'],\n",
        "    ['bulaşık'],\n",
        "    ['buzdolap'],\n",
        "    ['telefon'],\n",
        "    ['süpürge'],\n",
        "    ['sinema', 'sistem'],\n",
        "    ['fotoğraf', 'makine'],\n",
        "    ['klima'],\n",
        "    ['monitör'],\n",
        "    ['bilgisayar'],\n",
        "    ['televizyon'],\n",
        "    ['yazıcı'],\n",
        "    ['çamaşır'],\n",
        "    ['tablet']\n",
        "]\n",
        "\n",
        "# BERTopic modelini oluşturma\n",
        "topic_model = BERTopic(embedding_model=model, language='turkish', nr_topics=14, n_gram_range=(1, 2), verbose=True)\n",
        "\n",
        "# Metinleri bert tabanlı embedings'e dönüştürme\n",
        "embeddings = SentenceTransformer(model_name).encode(df.CustomerText.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Konu modelleme\n",
        "topics, _ = topic_model.fit_transform(df.CustomerText.tolist(), embeddings)\n",
        "\n",
        "# Belge başına olasılık hesaplama\n",
        "probs = topic_model.transform(df.CustomerText.tolist())\n",
        "\n",
        "# Embedings şekil doğrulama\n",
        "print(\"Embeddings'in şekli:\", embeddings.shape)\n",
        "representative_docs = topic_model.get_representative_docs()\n",
        "print(\"Her bir konu için temsili belgeler:\", representative_docs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvaqF3mzTequ"
      },
      "source": [
        "# model sonuçları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ganIEGu6_pTx"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WaAnljyH-XCF"
      },
      "outputs": [],
      "source": [
        "# 14 adet konu ve konuda geçen ilk 10 anahtar kelimesi\n",
        "for i in range(14):\n",
        "    print([i+1],'. konu: ',topic_model.get_topic(i) ,'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LWZ_LVYD_rAK"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XHxQfG_Zu3hu"
      },
      "outputs": [],
      "source": [
        "# çıkan konular ve konulara ait ilk 5 kelimenin olasılıklarının çubuk grafiklerler gösterimi\n",
        "topic_model.visualize_barchart(top_n_topics=99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNhvgAfxPvPM"
      },
      "source": [
        "#model değerlendirme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvqaAa8qT7Tl"
      },
      "source": [
        "## C_v (14 adet konu için)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym8xl98TOBvt"
      },
      "outputs": [],
      "source": [
        "# çıkan konular ve konulara ait ilk 5 kelimenin olasılıklarının çubuk grafiklerler gösterimi\n",
        "topic_model.visualize_barchart(top_n_topics=99)\n",
        "\n",
        "documents = pd.DataFrame({\"Document\": df.CustomerText,\n",
        "                          \"ID\": range(len(df.CustomerText)),\n",
        "                          \"Topic\": topics})\n",
        "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "vectorizer = topic_model.vectorizer_model\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "\n",
        "\n",
        "# tutarlılığı ölçmek (0 ile 1 arası skor / skorun 1'e yakın olması daha iyidir.)\n",
        "# C_v: Normalleştirilmiş noktasal karşılıklı bilgi (NPMI) ve kosinüs benzerliğini kullanan dolaylı bir doğrulama ölçüsüne dayanır.\n",
        "# Kayan pencere algoritmasına dayanır. En iyi kelimelerin tek kümeli segmentasyonu, her en iyi kelime vektörü ile tüm en iyi kelime vektörlerinin toplamı arasındaki benzerliğin hesaplanmasını sağlar.\n",
        "# Benzerlik ölçüsü olarak kosinüs kullanılır. Tutarlılık, bu benzerliklerin aritmetik ortalamasıdır.\n",
        "topic_words = [[words for words, _ in topic_model.get_topic(topic)]  # Remove the condition that filters out empty strings\n",
        "               for topic in range(topic_model.get_topic_info().shape[0])]  # Iterate over all topics including -1\n",
        "\n",
        "topic_words = [[words for words, _ in topic_model.get_topic(topic) if words]  # Filter out empty strings\n",
        "               for topic in range(topic_model.get_topic_info().shape[0])]  # Iterate over all topics including -1\n",
        "\n",
        "cv_model = CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,\n",
        "                                 corpus=corpus,\n",
        "                                 dictionary=dictionary,\n",
        "                                 coherence='c_v')\n",
        "cv_bert = cv_model.get_coherence()\n",
        "print('\\nTutarlılık skoru: ', cv_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99Ap75cgNyft"
      },
      "outputs": [],
      "source": [
        "#CHATGBT\n",
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Verilerin hazırlanması\n",
        "documents = pd.DataFrame({\"Document\": df.CustomerText,\n",
        "                          \"ID\": range(len(df.CustomerText)),\n",
        "                          \"Topic\": topics})\n",
        "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "vectorizer = topic_model.vectorizer_model\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "\n",
        "# Tutarlılığı ölçmek için Coherence Model hazırlığı\n",
        "topic_words = []\n",
        "for topic in range(len(topic_model.get_topics())):\n",
        "    words, _ = zip(*topic_model.get_topic(topic))\n",
        "    topic_words.append([word for word in words])\n",
        "\n",
        "# Coherence Model'i oluşturma\n",
        "cv_model = CoherenceModel(topics=topic_words,\n",
        "                          texts=tokens,\n",
        "                          corpus=corpus,\n",
        "                          dictionary=dictionary,\n",
        "                          coherence='c_v')\n",
        "cv_bert = cv_model.get_coherence()\n",
        "\n",
        "print('\\nTutarlılık skoru: ', cv_bert)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhbT6Tj_UAs1"
      },
      "source": [
        "## NPMI (14 adet konu için)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALZglJemCKLp"
      },
      "outputs": [],
      "source": [
        "# NPMI metriği, UCI'ın PMI yerine NPMI kullanılarak geliştirilmiş bir tutarlılık ölçüsüdür.\n",
        "# 1 ile 1 arasında sürekli bir değerdir. 1'e yaklaştıkça birlikte oluşumu gösterir.\n",
        "# 0'a yakınsa bağımsızlığı gösterir.\n",
        "npmi_model = CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,\n",
        "                                 corpus=corpus,\n",
        "                                 dictionary=dictionary,\n",
        "                                 coherence='c_npmi')\n",
        "npmi_bert = npmi_model.get_coherence()\n",
        "print('\\nTutarlılık skoru: ', npmi_bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOcCiQUoUZIt"
      },
      "source": [
        "## Topic Diversity (14 adet konu için)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iynsacUUdWv"
      },
      "outputs": [],
      "source": [
        "# Konu çeşitliliği açısından değerlendirmek için aşağıdaki kod yazılıyor.\n",
        "\n",
        "import numpy as np\n",
        "from rbo import rbo\n",
        "from scipy.spatial import distance\n",
        "from itertools import combinations\n",
        "from word_embeddings_rbo import word_embeddings_rbo\n",
        "\n",
        "def proportion_unique_words(topics, topk=10):\n",
        "    \"\"\"\n",
        "    compute the proportion of unique words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    topk: top k words on which the topic diversity will be computed\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than '+str(topk))\n",
        "    else:\n",
        "        unique_words = set()\n",
        "        for topic in topics:\n",
        "            unique_words = unique_words.union(set(topic[:topk]))\n",
        "        puw = len(unique_words) / (topk * len(topics))\n",
        "        return puw\n",
        "\n",
        "\n",
        "def irbo(topics, weight=0.9, topk=10):\n",
        "    \"\"\"\n",
        "    compute the inverted rank-biased overlap\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    weight: p (float), default 1.0: Weight of each\n",
        "        agreement at depth d:p**(d-1). When set\n",
        "        to 1.0, there is no weight, the rbo returns\n",
        "        to average overlap.\n",
        "    topk: top k words on which the topic diversity\n",
        "          will be computed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    irbo : score of the rank biased overlap over the topics\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than topk')\n",
        "    else:\n",
        "        collect = []\n",
        "        for list1, list2 in combinations(topics, 2):\n",
        "            word2index = get_word2index(list1, list2)\n",
        "            indexed_list1 = [word2index[word] for word in list1]\n",
        "            indexed_list2 = [word2index[word] for word in list2]\n",
        "            rbo_val = rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight)[2]\n",
        "            collect.append(rbo_val)\n",
        "        return 1 - np.mean(collect)\n",
        "\n",
        "\n",
        "def word_embedding_irbo(topics, word_embedding_model, weight=0.9, topk=10):\n",
        "    '''\n",
        "    compute the word embedding-based inverted rank-biased overlap\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    weight: p (float), default 1.0: Weight of each agreement at depth d:\n",
        "    p**(d-1). When set to 1.0, there is no weight, the rbo returns to average overlap.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    weirbo: word embedding-based inverted rank_biased_overlap over the topics\n",
        "    '''\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than topk')\n",
        "    else:\n",
        "        collect = []\n",
        "        for list1, list2 in combinations(topics, 2):\n",
        "            word2index = get_word2index(list1, list2)\n",
        "            index2word = {v: k for k, v in word2index.items()}\n",
        "            indexed_list1 = [word2index[word] for word in list1]\n",
        "            indexed_list2 = [word2index[word] for word in list2]\n",
        "            rbo_val = word_embeddings_rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight,\n",
        "                                          index2word=index2word, word2vec=word_embedding_model)[2]\n",
        "            collect.append(rbo_val)\n",
        "        return 1 - np.mean(collect)\n",
        "\n",
        "\n",
        "def pairwise_jaccard_diversity(topics, topk=10):\n",
        "    '''\n",
        "    compute the average pairwise jaccard distance between the topics\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    topk: top k words on which the topic diversity\n",
        "          will be computed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pjd: average pairwise jaccard distance\n",
        "    '''\n",
        "    dist = 0\n",
        "    count = 0\n",
        "    for list1, list2 in combinations(topics, 2):\n",
        "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
        "        dist = dist + js\n",
        "        count = count + 1\n",
        "    return dist/count\n",
        "\n",
        "\n",
        "def pairwise_word_embedding_distance(topics, word_embedding_model, topk=10):\n",
        "    \"\"\"\n",
        "    :param topk: how many most likely words to consider in the evaluation\n",
        "    :return: topic coherence computed on the word embeddings similarities\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than topk')\n",
        "    else:\n",
        "        count = 0\n",
        "        sum_dist = 0\n",
        "        for list1, list2 in combinations(topics, 2):\n",
        "            count = count+1\n",
        "            word_counts = 0\n",
        "            dist = 0\n",
        "            for word1 in list1[:topk]:\n",
        "                for word2 in list2[:topk]:\n",
        "                    dist = dist + distance.cosine(word_embedding_model.wv[word1], word_embedding_model.wv[word2])\n",
        "                    word_counts = word_counts + 1\n",
        "\n",
        "            dist = dist/word_counts\n",
        "            sum_dist = sum_dist + dist\n",
        "        return sum_dist/count\n",
        "\n",
        "\n",
        "def centroid_distance(topics, word_embedding_model, topk=10):\n",
        "    \"\"\"\n",
        "    :param topk: how many most likely words to consider in the evaluation\n",
        "    :return: topic coherence computed on the word embeddings similarities\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than topk')\n",
        "    else:\n",
        "        count = 0\n",
        "        for list1, list2 in combinations(topics, 2):\n",
        "            count = count + 1\n",
        "            centroid1 = np.zeros(word_embedding_model.vector_size)\n",
        "            centroid2 = np.zeros(word_embedding_model.vector_size)\n",
        "            for word1 in list1[:topk]:\n",
        "                centroid1 = centroid1 + word_embedding_model[word1]\n",
        "            for word2 in list2[:topk]:\n",
        "                centroid2 = centroid2 + word_embedding_model[word2]\n",
        "            centroid1 = centroid1 / len(list1[:topk])\n",
        "            centroid2 = centroid2 / len(list2[:topk])\n",
        "        return distance.cosine(centroid1, centroid2)\n",
        "\n",
        "\n",
        "def get_word2index(list1, list2):\n",
        "    words = set(list1)\n",
        "    words = words.union(set(list2))\n",
        "    word2index = {w: i for i, w in enumerate(words)}\n",
        "    return word2index\n",
        "\n",
        "\n",
        "\"\"\"Rank-biased overlap, a ragged sorted list similarity measure.\n",
        "See http://doi.acm.org/10.1145/1852102.1852106 for details. All functions\n",
        "directly corresponding to concepts from the paper are named so that they can be\n",
        "clearly cross-identified.\n",
        "The definition of overlap has been modified to account for ties. Without this,\n",
        "results for lists with tied items were being inflated. The modification itself\n",
        "is not mentioned in the paper but seems to be reasonable, see function\n",
        "``overlap()``. Places in the code which diverge from the spec in the paper\n",
        "because of this are highlighted with comments.\n",
        "The two main functions for performing an RBO analysis are ``rbo()`` and\n",
        "``rbo_dict()``; see their respective docstrings for how to use them.\n",
        "The following doctest just checks that equivalent specifications of a\n",
        "problem yield the same result using both functions:\n",
        "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
        "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
        "    >>> ans_rbo = _round(rbo(lst1, lst2, p=.9))\n",
        "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
        "    >>> ans_rbo_dict = _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
        "    >>> ans_rbo == ans_rbo_dict\n",
        "    True\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import math\n",
        "from bisect import bisect_left\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
        "RBO.__doc__ += \": Result of full RBO analysis\"\n",
        "RBO.min.__doc__ = \"Lower bound estimate\"\n",
        "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
        "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
        "\n",
        "\n",
        "def _round(obj):\n",
        "    if isinstance(obj, RBO):\n",
        "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
        "    else:\n",
        "        return round(obj, 3)\n",
        "\n",
        "\n",
        "def set_at_depth(lst, depth):\n",
        "    ans = set()\n",
        "    for v in lst[:depth]:\n",
        "        if isinstance(v, set):\n",
        "            ans.update(v)\n",
        "        else:\n",
        "            ans.add(v)\n",
        "    return ans\n",
        "\n",
        "\n",
        "def raw_overlap(list1, list2, depth):\n",
        "    \"\"\"Overlap as defined in the article.\n",
        "    \"\"\"\n",
        "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
        "    return len(set1.intersection(set2)), len(set1), len(set2)\n",
        "\n",
        "\n",
        "def overlap(list1, list2, depth):\n",
        "    \"\"\"Overlap which accounts for possible ties.\n",
        "    This isn't mentioned in the paper but should be used in the ``rbo*()``\n",
        "    functions below, otherwise overlap at a given depth might be > depth which\n",
        "    inflates the result.\n",
        "    There are no guidelines in the paper as to what's a good way to calculate\n",
        "    this, but a good guess is agreement scaled by the minimum between the\n",
        "    requested depth and the lengths of the considered lists (overlap shouldn't\n",
        "    be larger than the number of ranks in the shorter list, otherwise results\n",
        "    are conspicuously wrong when the lists are of unequal lengths -- rbo_ext is\n",
        "    not between rbo_min and rbo_min + rbo_res.\n",
        "    >>> overlap(\"abcd\", \"abcd\", 3)\n",
        "    3.0\n",
        "    >>> overlap(\"abcd\", \"abcd\", 5)\n",
        "    4.0\n",
        "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 2)\n",
        "    2.0\n",
        "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 3)\n",
        "    3.0\n",
        "    \"\"\"\n",
        "    ov = agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
        "    return ov\n",
        "    # NOTE: comment the preceding and uncomment the following line if you want\n",
        "    # to stick to the algorithm as defined by the paper\n",
        "    # return raw_overlap(list1, list2, depth)[0]\n",
        "\n",
        "\n",
        "def agreement(list1, list2, depth):\n",
        "    \"\"\"Proportion of shared values between two sorted lists at given depth.\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 1))\n",
        "    1.0\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 3))\n",
        "    0.667\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 4))\n",
        "    1.0\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 5))\n",
        "    0.8\n",
        "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 1))\n",
        "    0.667\n",
        "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 2))\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    len_intersection, len_set1, len_set2 = raw_overlap(list1, list2, depth)\n",
        "    return 2 * len_intersection / (len_set1 + len_set2)\n",
        "\n",
        "\n",
        "def cumulative_agreement(list1, list2, depth):\n",
        "    return (agreement(list1, list2, d) for d in range(1, depth + 1))\n",
        "\n",
        "\n",
        "def average_overlap(list1, list2, depth=None):\n",
        "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 1))\n",
        "    0.0\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 2))\n",
        "    0.0\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 3))\n",
        "    0.222\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 4))\n",
        "    0.292\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 5))\n",
        "    0.313\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 6))\n",
        "    0.317\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 7))\n",
        "    0.312\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    return sum(cumulative_agreement(list1, list2, depth)) / depth\n",
        "\n",
        "\n",
        "def rbo_at_k(list1, list2, p, depth=None):\n",
        "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
        "    # 0\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    d_a = enumerate(cumulative_agreement(list1, list2, depth))\n",
        "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
        "\n",
        "\n",
        "def rbo_min(list1, list2, p, depth=None):\n",
        "    \"\"\"Tight lower bound on RBO.\n",
        "    See equation (11) in paper.\n",
        "    >>> _round(rbo_min(\"abcdefg\", \"abcdefg\", .9))\n",
        "    0.767\n",
        "    >>> _round(rbo_min(\"abcdefgh\", \"abcdefg\", .9))\n",
        "    0.767\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    x_k = overlap(list1, list2, depth)\n",
        "    log_term = x_k * math.log(1 - p)\n",
        "    sum_term = sum(\n",
        "        p ** d / d * (overlap(list1, list2, d) - x_k) for d in range(1, depth + 1)\n",
        "    )\n",
        "    return (1 - p) / p * (sum_term - log_term)\n",
        "\n",
        "\n",
        "def rbo_res(list1, list2, p):\n",
        "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
        "    See equation (30) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
        "    ``rbo_res()`` should add up to 1, which is the case.\n",
        "    >>> _round(rbo_res(\"abcdefg\", \"abcdefg\", .9))\n",
        "    0.233\n",
        "    >>> _round(rbo_res(\"abcdefg\", \"abcdefghijklmnopqrstuvwxyz\", .9))\n",
        "    0.239\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l)\n",
        "    # since overlap(...) can be fractional in the general case of ties and f\n",
        "    # must be an integer --> math.ceil()\n",
        "    f = int(math.ceil(l + s - x_l))\n",
        "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
        "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
        "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
        "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
        "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
        "\n",
        "\n",
        "def rbo_ext(list1, list2, p):\n",
        "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
        "    See equation (32) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible.\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
        "    1.0\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
        "    0.9\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l)\n",
        "    x_s = overlap(list1, list2, s)\n",
        "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
        "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
        "    # properly (otherwise values > 1 will be returned)\n",
        "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
        "    sum1 = sum(p ** d * agreement(list1, list2, d) for d in range(1, l + 1))\n",
        "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
        "    term1 = (1 - p) / p * (sum1 + sum2)\n",
        "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
        "    return term1 + term2\n",
        "\n",
        "\n",
        "def rbo(list1, list2, p):\n",
        "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
        "    ``list`` arguments should be already correctly sorted iterables and each\n",
        "    item should either be an atomic value or a set of values tied for that\n",
        "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
        "    having examined rank k.\n",
        "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
        "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
        "    >>> _round(rbo(lst1, lst2, p=.9))\n",
        "    RBO(min=0.489, res=0.477, ext=0.967)\n",
        "    \"\"\"\n",
        "    if not 0 <= p <= 1:\n",
        "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
        "    args = (list1, list2, p)\n",
        "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
        "\n",
        "\n",
        "def sort_dict(dct, *, ascending=False):\n",
        "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
        "    Sorts in descending order by default, because the values are\n",
        "    typically scores, i.e. the higher the better. Specify\n",
        "    ``ascending=True`` if the values are ranks, or some sort of score\n",
        "    where lower values are better.\n",
        "    Ties are handled by creating sets of tied keys at the given position\n",
        "    in the sorted list.\n",
        "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
        "    True\n",
        "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
        "    True\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    items = []\n",
        "    # items should be unique, scores don't have to\n",
        "    for item, score in dct.items():\n",
        "        if not ascending:\n",
        "            score *= -1\n",
        "        i = bisect_left(scores, score)\n",
        "        if i == len(scores):\n",
        "            scores.append(score)\n",
        "            items.append(item)\n",
        "        elif scores[i] == score:\n",
        "            existing_item = items[i]\n",
        "            if isinstance(existing_item, set):\n",
        "                existing_item.add(item)\n",
        "            else:\n",
        "                items[i] = {existing_item, item}\n",
        "        else:\n",
        "            scores.insert(i, score)\n",
        "            items.insert(i, item)\n",
        "    return items\n",
        "\n",
        "\n",
        "def rbo_dict(dict1, dict2, p, *, sort_ascending=False):\n",
        "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
        "    Each dict maps items to be sorted to the score according to which\n",
        "    they should be sorted. The RBO analysis is then performed on the\n",
        "    resulting sorted lists.\n",
        "    The sort is descending by default, because scores are typically the\n",
        "    higher the better, but this can be overridden by specifying\n",
        "    ``sort_ascending=True``.\n",
        "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
        "    >>> _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
        "    RBO(min=0.489, res=0.477, ext=0.967)\n",
        "    \"\"\"\n",
        "    list1, list2 = (\n",
        "        sort_dict(dict1, ascending=sort_ascending),\n",
        "        sort_dict(dict2, ascending=sort_ascending),\n",
        "    )\n",
        "    return rbo(list1, list2, p)\n",
        "\n",
        "\n",
        "if __name__ in (\"__main__\", \"__console__\"):\n",
        "    import doctest\n",
        "\n",
        "    doctest.testmod()\n",
        "\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import math\n",
        "from bisect import bisect_left\n",
        "from collections import namedtuple\n",
        "from collections import OrderedDict\n",
        "\n",
        "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
        "RBO.__doc__ += \": Result of full RBO analysis\"\n",
        "RBO.min.__doc__ = \"Lower bound estimate\"\n",
        "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
        "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
        "\n",
        "def _round(obj):\n",
        "    if isinstance(obj, RBO):\n",
        "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
        "    else:\n",
        "        return round(obj, 3)\n",
        "\n",
        "\n",
        "def set_at_depth(lst, depth):\n",
        "    ans = set()\n",
        "    for v in lst[:depth]:\n",
        "        if isinstance(v, set):\n",
        "            ans.update(v)\n",
        "        else:\n",
        "            ans.add(v)\n",
        "    return ans\n",
        "\n",
        "\n",
        "def embeddings_overlap(list1, list2, depth, index2word, word2vec):\n",
        "    #set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
        "    #return len(set1.intersection(set2)), len(set1), len(set2)\n",
        "\n",
        "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
        "    word_list1 = [index2word[index] for index in list1]\n",
        "    word_list2 = [index2word[index] for index in list2]\n",
        "\n",
        "    similarities = {}\n",
        "    for w1 in word_list1[:depth]:\n",
        "        for w2 in word_list2[:depth]:\n",
        "            similarities[(w1,w2)] = word2vec.similarity(w1, w2)\n",
        "\n",
        "    similarities = OrderedDict(sorted(similarities.items(), key=lambda x: -x[1]))\n",
        "\n",
        "    e_ov = 0\n",
        "    key_list = list(similarities.keys())\n",
        "    for k in key_list:\n",
        "        if k in similarities.keys():\n",
        "            #print(k, similarities[k])\n",
        "            e_ov = e_ov + similarities[k]\n",
        "            similarities = {save_k: v for save_k, v in similarities.items()\n",
        "                            if save_k[0] != k[0] and save_k[1] != k[1]}\n",
        "    #e_ov = 1\n",
        "    #print(\"****\")\n",
        "    return e_ov, len(set1), len(set2)\n",
        "\n",
        "\n",
        "def overlap(list1, list2, depth, index2word, word2vec):\n",
        "    #return agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
        "    # NOTE: comment the preceding and uncomment the following line if you want\n",
        "    # to stick to the algorithm as defined by the paper\n",
        "    ov = embeddings_overlap(list1, list2, depth, index2word, word2vec)[0]\n",
        "    return ov\n",
        "\n",
        "\n",
        "def agreement(list1, list2, depth, index2word, word2vec):\n",
        "    \"\"\"Proportion of shared values between two sorted lists at given depth.\"\"\"\n",
        "    len_intersection, len_set1, len_set2 = embeddings_overlap(list1, list2, depth, index2word, word2vec)\n",
        "    return 2 * len_intersection / (len_set1 + len_set2)\n",
        "\n",
        "\n",
        "def cumulative_agreement(list1, list2, depth, index2word, word2vec):\n",
        "    return (agreement(list1, list2, d, index2word, word2vec) for d in range(1, depth + 1))\n",
        "\n",
        "\n",
        "def average_overlap(list1, list2, index2word, word2vec, depth=None):\n",
        "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    return sum(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec)) / depth\n",
        "\n",
        "\n",
        "def rbo_at_k(list1, list2, p, index2word, word2vec, depth=None):\n",
        "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
        "    # 0\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    d_a = enumerate(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec))\n",
        "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
        "\n",
        "\n",
        "def rbo_min(list1, list2, p, index2word, word2vec, depth=None):\n",
        "    \"\"\"Tight lower bound on RBO.\n",
        "    See equation (11) in paper.\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    x_k = overlap(list1, list2, depth, index2word, word2vec)\n",
        "    log_term = x_k * math.log(1 - p)\n",
        "    sum_term = sum(\n",
        "        p ** d / d * (overlap(list1, list2, d, index2word, word2vec=word2vec) - x_k) for d in range(1, depth + 1)\n",
        "    )\n",
        "    return (1 - p) / p * (sum_term - log_term)\n",
        "\n",
        "\n",
        "def rbo_res(list1, list2, p, index2word, word2vec):\n",
        "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
        "    See equation (30) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
        "    ``rbo_res()`` should add up to 1, which is the case.\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
        "    # since overlap(...) can be fractional in the general case of ties and f\n",
        "    # must be an integer --> math.ceil()\n",
        "    f = int(math.ceil(l + s - x_l))\n",
        "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
        "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
        "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
        "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
        "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
        "\n",
        "\n",
        "def rbo_ext(list1, list2, p, index2word, word2vec):\n",
        "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
        "    See equation (32) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible.\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
        "    1.0\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
        "    0.9\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
        "    x_s = overlap(list1, list2, s, index2word, word2vec)\n",
        "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
        "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
        "    # properly (otherwise values > 1 will be returned)\n",
        "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
        "    sum1 = sum(p ** d * agreement(list1, list2, d, index2word=index2word, word2vec=word2vec)\n",
        "               for d in range(1, l + 1))\n",
        "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
        "    term1 = (1 - p) / p * (sum1 + sum2)\n",
        "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
        "    return term1 + term2\n",
        "\n",
        "\n",
        "def word_embeddings_rbo(list1, list2, p, index2word, word2vec):\n",
        "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
        "    ``list`` arguments should be already correctly sorted iterables and each\n",
        "    item should either be an atomic value or a set of values tied for that\n",
        "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
        "    having examined rank k.\n",
        "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
        "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
        "    >>> _round(rbo(lst1, lst2, p=.9))\n",
        "    RBO(min=0.489, res=0.477, ext=0.967)\n",
        "    \"\"\"\n",
        "    if not 0 <= p <= 1:\n",
        "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
        "    args = (list1, list2, p, index2word, word2vec)\n",
        "\n",
        "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
        "\n",
        "\n",
        "def sort_dict(dct, *, ascending=False):\n",
        "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
        "    Sorts in descending order by default, because the values are\n",
        "    typically scores, i.e. the higher the better. Specify\n",
        "    ``ascending=True`` if the values are ranks, or some sort of score\n",
        "    where lower values are better.\n",
        "    Ties are handled by creating sets of tied keys at the given position\n",
        "    in the sorted list.\n",
        "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
        "    True\n",
        "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
        "    True\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    items = []\n",
        "    # items should be unique, scores don't have to\n",
        "    for item, score in dct.items():\n",
        "        if not ascending:\n",
        "            score *= -1\n",
        "        i = bisect_left(scores, score)\n",
        "        if i == len(scores):\n",
        "            scores.append(score)\n",
        "            items.append(item)\n",
        "        elif scores[i] == score:\n",
        "            existing_item = items[i]\n",
        "            if isinstance(existing_item, set):\n",
        "                existing_item.add(item)\n",
        "            else:\n",
        "                items[i] = {existing_item, item}\n",
        "        else:\n",
        "            scores.insert(i, score)\n",
        "            items.insert(i, item)\n",
        "    return items\n",
        "\n",
        "\n",
        "def rbo_dict(dict1, dict2, p, index2word, word2vec, *, sort_ascending=False):\n",
        "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
        "    Each dict maps items to be sorted to the score according to which\n",
        "    they should be sorted. The RBO analysis is then performed on the\n",
        "    resulting sorted lists.\n",
        "    The sort is descending by default, because scores are typically the\n",
        "    higher the better, but this can be overridden by specifying\n",
        "    ``sort_ascending=True``.\n",
        "    \"\"\"\n",
        "    list1, list2 = (\n",
        "        sort_dict(dict1, ascending=sort_ascending),\n",
        "        sort_dict(dict2, ascending=sort_ascending),\n",
        "    )\n",
        "    return word_embeddings_rbo(list1, list2, p, index2word, word2vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3TqO9zJIUfAd"
      },
      "outputs": [],
      "source": [
        "from diversity_metrics import *\n",
        "\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jQ3E81UgIN"
      },
      "outputs": [],
      "source": [
        "word_list1 =['internet','servis','telefon', 'kanal', 'uydu','smart', 'sıkıntı', 'görüntü','destek','garanti']\n",
        "word_list2= ['ekran', 'servis', 'cep','mesaj','telefon arıza','es', 'gerekiyor','bilgi','telefon birim','makine']\n",
        "word_list3 =['telefon','ekran', 'ses','uygulama','geri','sıkıntı','sorun','mail','cep','ediyorum']\n",
        "word_list4= ['servis','buzdolap','klima','ses','lira','yeni','sorun','ediyorum','içinde','çamaşır makine']\n",
        "word_list5 =['inç','kurulum','montaj','duvara','ediyorum','umuz','almak','bilgi','garanti','televizyon']\n",
        "word_list6= ['servis','çamaşır makine','buzdolap','ediyorum','kaç','saat','garanti','bekliyorum','içinde','evde']\n",
        "word_list7 =['ses','servis','nereden','buzdolap','klima','ederiz','eşiyim','çamaşır makine','olun','sağ']\n",
        "word_list8= ['çamaşır makine','ses','garanti','program','problem','suyu','bubble','kilo','devam','ediyorum']\n",
        "word_list9 =['telefon','galaxy','uygulama','servis','garanti','sıkıntı','cep','normal','kaç','oraya']\n",
        "word_list10= ['şarj','galaxy tab','vardı','uygulama','ediyorum','oluyor','ses','internet','geri','ekran']\n",
        "word_list11=['telefon','imei','ses','ediyorum','gmail','şifre','hala','yaptım','kom','mail adresi']\n",
        "word_list12= ['telefon','ekran','uygulama','ses','garanti','sorun','kendi','sıkıntı','istiyorum','bilemiyorum']\n",
        "word_list13 =['youtube','telefon','uygulama','güncelleme','sorun','şifre','çıkıyor','destek','normal','kodu']\n",
        "word_list14= ['buzdolap','servis','motor garanti','klima','evde','sene','soğutmuyor','elektrik','çamaşır makine','devam']\n",
        "\n",
        "topics = [word_list1, word_list2, word_list3, word_list4, word_list5, word_list6, word_list7, word_list8, word_list9, word_list10, word_list11, word_list12,\n",
        "          word_list13, word_list14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4ouMwA9UlyU"
      },
      "outputs": [],
      "source": [
        "print(\"puw:\",proportion_unique_words(topics, topk=10)) # Benzersiz kelimelerin oranı(puw)\n",
        "print(\"jd:\", pairwise_jaccard_diversity(topics, topk=10)) # ortalama İkili Jaccard Mesafesi(jd)\n",
        "print(\"irbo p=0.5:\",irbo(topics, weight=0.5, topk=10)) # Ters Sıra Önyargılı Örtüşme(irbo) ağırlık=0,5\n",
        "print(\"irbo p=0.9:\",irbo(topics, weight=0.9, topk=10)) # Ters Sıra Önyargılı Örtüşme(irbo) ağırlık=0,9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sep021Vcau2W"
      },
      "source": [
        "## Konu sayısına göre değerlendirmeler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y9kp9eXazZO"
      },
      "outputs": [],
      "source": [
        "# konu sayısı değiştikçe skor da değişmektedir bu sebeple konu sayısının değişmesiyle skorun değişmesini gözlemlenir ve en iyi konu saysına karar verilir.\n",
        "import time\n",
        "def compute_coherence_values(dictionary, topics, corpus, texts, limit, start, step):\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    cv_values = []\n",
        "    cnpmi_values=[]\n",
        "    time_vals=[]\n",
        "    model2_list = []\n",
        "\n",
        "    for nr_topics in range(start, limit, step):\n",
        "        model2 = BERTopic(embedding_model=model, diversity=0.7,\n",
        "                       language='multilingual', nr_topics=nr_topics, n_gram_range=(1,2), verbose=True,\n",
        "                       calculate_probabilities=True, seed_topic_list=[['ankastre','fırın','set üstü','davlumbaz','mikro dalga'],['bulaşık'],['buzdolap'],['telefon'],['süpürge'],['sinema sistem'],\n",
        "                                                                                    ['fotoğraf makine'],['klima'],['monitör'],['bilgisayar'],['televizyon'],['yazıcı'],['çamaşır'],['tablet']])\n",
        "        model2_list.append(model2)\n",
        "        topics, probs = model2.fit_transform(df.CustomerText)\n",
        "\n",
        "        cleaned_docs = model2._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "        vectorizer = model2.vectorizer_model\n",
        "        analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "        words = vectorizer.get_feature_names()\n",
        "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "        dictionary = corpora.Dictionary(tokens)\n",
        "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "        topic_words = [[words for words, _ in model2.get_topic(topic)]\n",
        "                    for topic in range(len(set(topics))-1)]\n",
        "\n",
        "        cv_model2 = CoherenceModel(topics=topic_words, texts=tokens, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
        "        cnpmi_model2 =CoherenceModel(topics=topic_words, texts=tokens, corpus=corpus, dictionary=dictionary, coherence='c_npmi')\n",
        "\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "\n",
        "        cv_values.append(cv_model2.get_coherence())\n",
        "        cnpmi_values.append(cnpmi_model2.get_coherence())\n",
        "        time_vals.append((end_time-start_time)/60)\n",
        "\n",
        "    return model2_list, cv_values, cnpmi_values, time_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5JIPvJxb-J-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "x = range(5, 20, 3)\n",
        "model2_list, cv_values, cnpmi_values, time_vals= compute_coherence_values(dictionary=dictionary, topics= topic_words, corpus=corpus, texts=tokens,\n",
        "                                                                         start=5, limit=20, step=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKbxnezCb_T_"
      },
      "outputs": [],
      "source": [
        "timeee=[]\n",
        "timeee.append(time_vals[0])\n",
        "for i in range(0,4):\n",
        "    sure=time_vals[i+1]-time_vals[i]\n",
        "    timeee.append(sure)\n",
        "\n",
        "sonuc_df = pd.DataFrame(data = {'Konu sayısı': x, 'c_v skoru(0,1)': cv_values, 'NPMI skoru(-1,1)': cnpmi_values, 'Süre(dakika)': timeee})\n",
        "sonuc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLZMKAS2u66B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "start=5; limit=20; step=3;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, cv_values, 'o-')\n",
        "plt.xlabel(\"Number of topics\")\n",
        "plt.ylabel(\"c_v coherence score(0,1)\")\n",
        "plt.title('Higher is better')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGO3FL2yvAPS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "start=5; limit=20; step=3;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, cnpmi_values, 'o-')\n",
        "plt.xlabel(\"Number of topics\")\n",
        "plt.ylabel(\"NPMI score(-1,1)\")\n",
        "plt.title('Higher is better')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMj-la2TvCSi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "start=5; limit=20; step=3;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, timeee, 'o-')\n",
        "plt.xlabel(\"Number of topics\")\n",
        "plt.ylabel(\"Time(min)\")\n",
        "plt.title('Lower is better')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YycJZmYJFvTw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGsRdGN9FyWX"
      },
      "source": [
        "# Yeni Bölüm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekxOyofF3zP"
      },
      "outputs": [],
      "source": [
        "topics_seri = pd.Series(topics)\n",
        "\n",
        "# veriseti oluşturulur.\n",
        "df_konular = pd.DataFrame({'konu': topics, 'yazi': df.CustomerText})\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "# Assume df_konular is your DataFrame with 'yazi' and 'konu' columns\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=100000)  # Adjust num_words as needed\n",
        "tokenizer.fit_on_texts(df_konular['yazi'])\n",
        "X = tokenizer.texts_to_sequences(df_konular['yazi'])\n",
        "\n",
        "# One-hot encode the labels\n",
        "Y = pd.get_dummies(df_konular['konu']).values\n",
        "\n",
        "# Pad sequences to ensure consistent input shape\n",
        "max_tokens = 100  # Example max_tokens, adjust as per your requirements\n",
        "X_pad = pad_sequences(X, maxlen=max_tokens)\n",
        "\n",
        "# Resample the data to balance the classes\n",
        "rus = RandomOverSampler(random_state=0, sampling_strategy=\"minority\")\n",
        "X_resampled, Y_resampled = rus.fit_resample(X_pad, Y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_resampled, Y_resampled, test_size=0.25, random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZZ91di6F9Gw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import  TfidfVectorizer # Metinleri makinenin anlayacağı dile çevirmek için vektörlere çevirmeliyiz. Bu yüzden bir ham doküman koleksiyonunu TF-IDF özelliklerinin bir matrisine dönüştürmek için TfidfVectorizer kullanılır.\n",
        "# TF: Term Frequency=terim frekansı: bir dokümanda belirli bir terimden kaç tane olduğunu bulur. / IDF: Inverse Document Frequency=Ters doküman frekansı: belirli terimin kaç farklı dokümanda geçtiğini hesaplar.\n",
        "# Tf-idf ise belirli terimin kaç farklı dokümanda kaç kere geçtiğine önem verir. Bu yüzden tf-idf kullanıldı.\n",
        "from sklearn.model_selection import train_test_split # Veriyi eğitim ve test diye ayırmak için modül\n",
        "from sklearn import metrics # performans değerlendirmesi için metrik modülü yüklenir\n",
        "from sklearn.metrics import accuracy_score # doğruluk skoru hesaplamak için yüklenir.\n",
        "\n",
        "dataDoc = df_konular['yazi'].values.tolist() # Metin verileri girdi olarak seçilir.\n",
        "dataClass = df_konular['konu'].values.tolist() # konu verileri çıktı olarak seçilir.\n",
        "\n",
        "# Tfidf skorlama yöntemini kullanarak veriyi sayısallaştırmadan önce eğitim ve test olarak veriyi ayırıyoruz.\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataDoc, dataClass, test_size = 0.25, random_state = 42)\n",
        "\n",
        "#tfidf işlemi\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=3) # min_df: Nadiren görünen terimleri göz ardı etmek için kullanılır. Şu anda bir terim 3 dokümandan az geçiyorsa göz ardı edilecek.\n",
        "\n",
        "# metin verileri makinenin anlayacağı dile vektörlere dönüştürülür.\n",
        "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBBMB8l_GEsI"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression # Lojistik regresyon kullanmak için yüklenir.\n",
        "#lr = LogisticRegression(solver='sag', random_state=18,multi_class='multinomial', max_iter=782) # saga çözümleyicisi sag'ın gelişmiş bir varyantıdır. Temelinde stokastik ortalama gradyan inişi baz alır.\n",
        "lr= LogisticRegression(\n",
        "                max_iter=66,\n",
        "                C=0.4,\n",
        "                penalty='none',\n",
        "                class_weight=None,\n",
        "                dual=False,\n",
        "                fit_intercept=True,\n",
        "                n_jobs=-1,\n",
        "                random_state=15,\n",
        "                solver='saga',\n",
        "                tol=1e-3\n",
        "                )\n",
        "lr_clf = lr.fit(x_train, y_train) # model eğitilir.\n",
        "pred_test_lr = lr_clf.predict(x_test) # tahmin yapılır\n",
        "\n",
        "\n",
        "end = datetime.now()\n",
        "print(end-start)\n",
        "print(\"****Bitti****\")\n",
        "\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_lr = lr_clf.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('Lojistik Regresyon Model eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_lr)))\n",
        "\n",
        "# Test skoru\n",
        "print('Lojistik Regresyon Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_lr)))\n",
        "print('Lojistik Regresyon Model test veri seti f1: {0:0.4f}'.format(metrics.f1_score(y_test, pred_test_lr, average='weighted')))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import metrics # performans değerlendirmesi için metrik modülü yüklenir.\n",
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_lr)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır.\n",
        "\n",
        "accuracy = accuracy_score(y_test,pred_test_lr)\n",
        "precision = precision_score(y_test,pred_test_lr, average='weighted')\n",
        "recall = recall_score(y_test,pred_test_lr, average='weighted')\n",
        "f1 = f1_score(y_test,pred_test_lr, average='weighted')\n",
        "\n",
        "print(f\"loj Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw-y9iA7GIjg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lH9EvvdF0WW"
      },
      "source": [
        "# Yeni Bölüm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XigUAJ1lk482"
      },
      "source": [
        "## 14 ana konu için sınıflandırma algoritmaları"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TilZ6E_rlK7W"
      },
      "outputs": [],
      "source": [
        "# yeni veriseti oluşturup sınıflandırma yapılır. Bunun için konu modellemenin çıkarttığı konular(topics) seriye dönüştürülür.\n",
        "topics_seri = pd.Series(topics)\n",
        "topics_seri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BXgwD2NimApo"
      },
      "outputs": [],
      "source": [
        "df.CustomerText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FGyNyt1rk35a"
      },
      "outputs": [],
      "source": [
        "# veriseti oluşturulur.\n",
        "df_konular = pd.DataFrame({'konu': topics, 'yazi': df.CustomerText})\n",
        "df_konular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EwFyCA90mETO"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer # Makinenin yazıyı anlayabilmesi için Tokenizer ile veri setinde kullanılan en çok 100000 kelime ile bir sözlük oluşturulur.\n",
        "import numpy as np # Matematiksel işlemler için gerekli numpy kütüphanesi\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100000) # sözlük oluşturulur.\n",
        "tokenizer.fit_on_texts(df_konular['yazi']) # yazılar tokene dönüştürülür.\n",
        "\n",
        "# toplam token sayısı\n",
        "num_tokens = [len(tokens) for tokens in df_konular['yazi']]\n",
        "num_tokens = np.array(num_tokens)\n",
        "\n",
        "# max token sayısı\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens\n",
        "\n",
        "# Sinir ağımız bizden girdileri eşit boyutta istiyor. Gerçek hayattaki problemlerde bu çok zordur. Bu yüzden verileri aynı boyuta getirmek için pad_sequences kullanılır.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df_konular['yazi']) # cümleleri sözlükteki kelimelerin karşılığına çevrilir.\n",
        "Y = pd.get_dummies(df_konular['konu']).values # çıktı verilerimiz anlayacağı hale getirdik.\n",
        "X_pad = pad_sequences(X, maxlen=max_tokens) # pad_sequences ile aynı boyuta getirilir. uzunluk olarak da önceden hesapladığımız cümle boyutu max_tokens koydum.\n",
        "\n",
        "# verimiz dengesiz olduğu için RandomOverSampler ile verimizi yeniden örnekliyoruz.\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "rus = RandomOverSampler(random_state = 0, sampling_strategy = \"minority\") # RandomUnderSampler parametreleri belirlenerek değişkene atanır. sampling_strategy, örnekleme stratejisi, minority olarak belirlendi. Çünkü verilerimiz dengesiz olduğu için azınlıkları(minority) yeniden örnekleyerek veriyi dengelemeye çalışıyoruz.\n",
        "x_rus, y_rus = rus.fit_resample(X_pad, Y) # veriler yeniden örneklendirilir.\n",
        "print(x_rus.shape, y_rus.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split # verileri eğitim ve test diye bölünür.\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_pad, Y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "556QIRidmGru"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # yapay sinir ağı kullanmak için keras yüklenir.\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_ker = keras.Sequential() # Bir sıralı(Sequential) model, tek girdi ve tek çıktı katmanın olduğu derin öğrenme modeli oluşturmaya yarar.\n",
        "model_ker.add(keras.layers.Embedding(100000, 100, input_length=max_tokens)) # yazıyı vektörlere çevirir. Tanımı: Pozitif tamsayı (indeksleri) sabit boyutta yoğun vektörlere dönüştürür.\n",
        "model_ker.add(keras.layers.LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
        "model_ker.add(keras.layers.Dense(7, activation='softmax')) # çıktı katmanı. SoftMAX aktivasyon fonksiyonu, göreceli olasılıkları hesaplar. çoklu sınıf olduğu için softmax kullanılır. ağın son katmanı için aktivasyon olarak kullanılır, çünkü sonuç bir olasılık dağılımı olarak yorumlanabilir. softmanx kullanılmasının sebebi modelin multi-class ve bir doğru cevabın olmasındandır.\n",
        "\n",
        "model_ker.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # çok sınıflı sınıflandırma problemi olduğu için categorical_crossentropy kullanıldı.\n",
        "\n",
        "epochs = 5 # Algoritmasının tüm eğitim veri kümesinde kaç kere çalışacağını belirtir.\n",
        "batch_size = 512 # batch, bir veya daha fazla örnek üzerinde yineleme yapan ve tahminlerde bulunan için bir döngü sayısıdır. her örnekten sonra tahminler beklenen çıktılarla karşılaştırılır ve bir hata hesaplanır. hataya göre tahmini iyileştirmeye çalışır.\n",
        "\n",
        "callback = (keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001))# model çalıştırılır(validation loss'u gözle eğer 3 epochdur(devirdir) en az 0.0001 gelişmiyorsa eğitimi durdur.)\n",
        "\n",
        "history = model_ker.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=[x_test,y_test],callbacks=[callback]) # model çalıştırılır\n",
        "\n",
        "# modelimizin test verileri doğruluğu\n",
        "accr = model_ker.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZVlxhibmJUe"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import  TfidfVectorizer # Metinleri makinenin anlayacağı dile çevirmek için vektörlere çevirmeliyiz. Bu yüzden bir ham doküman koleksiyonunu TF-IDF özelliklerinin bir matrisine dönüştürmek için TfidfVectorizer kullanılır.\n",
        "# TF: Term Frequency=terim frekansı: bir dokümanda belirli bir terimden kaç tane olduğunu bulur. / IDF: Inverse Document Frequency=Ters doküman frekansı: belirli terimin kaç farklı dokümanda geçtiğini hesaplar.\n",
        "# Tf-idf ise belirli terimin kaç farklı dokümanda kaç kere geçtiğine önem verir. Bu yüzden tf-idf kullanıldı.\n",
        "from sklearn.model_selection import train_test_split # Veriyi eğitim ve test diye ayırmak için modül\n",
        "from sklearn import metrics # performans değerlendirmesi için metrik modülü yüklenir\n",
        "from sklearn.metrics import accuracy_score # doğruluk skoru hesaplamak için yüklenir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9pQ54mBqvK8"
      },
      "outputs": [],
      "source": [
        "dataDoc = df_konular['yazi'].values.tolist() # Metin verileri girdi olarak seçilir.\n",
        "dataClass = df_konular['konu'].values.tolist() # konu verileri çıktı olarak seçilir.\n",
        "\n",
        "# Tfidf skorlama yöntemini kullanarak veriyi sayısallaştırmadan önce eğitim ve test olarak veriyi ayırıyoruz.\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataDoc, dataClass, test_size = 0.25, random_state = 42)\n",
        "\n",
        "#tfidf işlemi\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=3) # min_df: Nadiren görünen terimleri göz ardı etmek için kullanılır. Şu anda bir terim 3 dokümandan az geçiyorsa göz ardı edilecek.\n",
        "\n",
        "# metin verileri makinenin anlayacağı dile vektörlere dönüştürülür.\n",
        "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKY4NzL-rAg0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier # Lojistik regresyon kullanmak için yüklenir.\n",
        "lrsgd = SGDClassifier(loss=\"log\", max_iter=1000, alpha=0.0001, random_state=42) # saga çözümleyicisi sag'ın gelişmiş bir varyantıdır. Temelinde stokastik ortalama gradyan inişi baz alır.\n",
        "lrsgd_clf = lrsgd.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "pred_test_lrsgd = lrsgd_clf.predict(x_test_tfidf) # tahmin yapılır\n",
        "\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_lrsgd = lrsgd_clf.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('SGD''li Lojistik Regresyon Model eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_lrsgd)))\n",
        "\n",
        "# Test skoru\n",
        "print('SGD''li Lojistik Regresyon Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_lrsgd)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5jMNNTRrFus"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_lrsgd, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oRU2SywrBH0"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression # Lojistik regresyon kullanmak için yüklenir.\n",
        "lr = LogisticRegression(solver='saga', random_state=42,multi_class='multinomial', max_iter=1000) # saga çözümleyicisi sag'ın gelişmiş bir varyantıdır. Temelinde stokastik ortalama gradyan inişi baz alır.\n",
        "lr_clf = lr.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "pred_test_lr = lr_clf.predict(x_test_tfidf) # tahmin yapılır\n",
        "\n",
        "end = datetime.now()\n",
        "print(end-start)\n",
        "print(\"****Bitti****\")\n",
        "\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_lr = lr_clf.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('Lojistik Regresyon Model eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_lr)))\n",
        "\n",
        "# Test skoru\n",
        "print('Lojistik Regresyon Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_lr)))\n",
        "print('Lojistik Regresyon Model test veri seti f1: {0:0.4f}'.format(metrics.f1_score(y_test, pred_test_lr, average='weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjKym3_UrHKU"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics # performans değerlendirmesi için metrik modülü yüklenir.\n",
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_lr, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyVielSvrDJE"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "\n",
        "import lightgbm as lgbm # lightgbm kullanmak için yüklenir.\n",
        "\n",
        "# lightgbm modeli tanımlanır.\n",
        "clf_LGBM = lgbm.LGBMClassifier(min_data_in_leaf=10,feature_fraction=0.5, learning_rate=0.01, random_state=42, max_depth=3, num_leaves=7, objective='multiclass')\n",
        "# learning_rate=0.01> öğrenme adım sayısını doğruluğu artırmak adına düşürdüm.\n",
        "# overfitting düşürmek için min_data_in_leaf kullandım. Böylece her yaprakta bulunacak minimum veriyi çoğaltarak bir yaprağın çok spesifik hale gelmesini engelledim.\n",
        "# overfitting düşürmek için feature_fraction kullandım. Böylece her iterasyonda rastgele seçilecek özniteliklerin oranı belirlendi.\n",
        "\n",
        "clf_LGBM.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "\n",
        "pred_test_LGBM = clf_LGBM.predict(x_test_tfidf) # Tahmin yapılır.\n",
        "\n",
        "end = datetime.now()\n",
        "print(end-start)\n",
        "print(\"****Bitti****\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTomHpOnrKns"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics # performans değerlendirmesi için metrik modülü yüklenir.\n",
        "\n",
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_LGBM, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MKmefy5rM1U"
      },
      "outputs": [],
      "source": [
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_LGBM = clf_LGBM.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "\n",
        "\n",
        "print('LightGBM Model eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_LGBM)))\n",
        "\n",
        "# Test skoru\n",
        "print('LightGBM Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_LGBM)))\n",
        "print('lightgbm Model test veri seti f1: {0:0.4f}'.format(metrics.f1_score(y_test, pred_test_LGBM, average='weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_vmBdNArOYO"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf_sgd = SGDClassifier(loss=\"hinge\", alpha=0.001, random_state=42) # sgd optimizasyonu ile lineer kernel SVC\n",
        "# alpha, iki amaca hizmet eder. Hem C gibi bir düzenleme parametresi hem de varsayılan zamanlama kapsamındaki ilk öğrenme oranıdır(learning_rate).\n",
        "# hinge loss, sınıflandırma sınırından kayıp hesaplamasında mesafeyi içeren belirli bir kayıp fonksiyonudur. SVM'de kullanılır.\n",
        "clf_sgd.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "pred_test_svcsgd = clf_sgd.predict(x_test_tfidf) # tahmin ettirilir.\n",
        "\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_svcsgd = clf_sgd.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('svcsgd eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_svcsgd)))\n",
        "\n",
        "# Test skoru\n",
        "print('svcsgd Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_svcsgd)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvOnx_ZTrUsc"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_svcsgd, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw3GR4o4rSkc"
      },
      "outputs": [],
      "source": [
        "clf_lin = svm.SVC(kernel='linear', C=0.9, decision_function_shape='ovo', random_state=42) # lineer kernel SVC\n",
        "# C, SVC için bir parametredir ve verinin yanlış sınıflandırılması için cezadır. C düştükçe sınıflandırıcı yanlış sınıflandırılmış veri noktalarını göz ardı eder.\n",
        "clf_lin.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "pred_test_svclin = clf_lin.predict(x_test_tfidf) # tahmin ettirilir.\n",
        "\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_svclin = clf_lin.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('svclin eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_svclin)))\n",
        "\n",
        "# Test skoru\n",
        "print('svclin Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_svclin)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFZ3eMWSrWE0"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_svclin, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coRjHKmirX18"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "\n",
        "from sklearn.svm import LinearSVC # Libsvm kütüphanesi yerine Liblinear kütüphanesini kullanır. Daha esnektir.\n",
        "lrsvc = LinearSVC(C=0.2, random_state=42) # C, SVC için bir parametredir ve verinin yanlış sınıflandırılması için cezadır. C düştükçe sınıflandırıcı yanlış sınıflandırılmış veri noktalarını göz ardı eder.\n",
        "lrsvc_clf = lrsvc.fit(x_train_tfidf, y_train) # model eğitilir.\n",
        "pred_test_lrsvc = lrsvc_clf.predict(x_test_tfidf) # tahmin yapılır\n",
        "\n",
        "end = datetime.now()\n",
        "print(end-start)\n",
        "print(\"****Bitti****\")\n",
        "# Modelimizde overfitting olmuş mu diye bakmak için eğitim skoru ve test skoru karşılaştırılır. eğer değerler yakınsa overfitting yoktur denir.\n",
        "# Eğitim skoru\n",
        "pred_train_lrsvc = lrsvc_clf.predict(x_train_tfidf) # x eğitim verileri de karşılaştırma yapmak için tahmin edilir.\n",
        "print('lineer svc Model eğitim veri seti doğruluğu: {0:0.4f}'. format(metrics.accuracy_score(y_train, pred_train_lrsvc)))\n",
        "\n",
        "# Test skoru\n",
        "print('lineer svc Model test veri seti doğruluğu: {0:0.4f}'.format(metrics.accuracy_score(y_test, pred_test_lrsvc)))\n",
        "print('lineer svc Model test veri seti f1: {0:0.4f}'.format(metrics.f1_score(y_test, pred_test_lrsvc, average='weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLuYmfrtrY8M"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_true = y_test, y_pred=pred_test_lrsvc, zero_division=0)) # y_true: doğru hedef değişkenleridir. y_pred: sınıflandırma modelinden çıkan tahmin sonuçlarıdır."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}